#!/gpfs0/arazi/projects/miniconda/bin/python3


###
###  Import libraries
###

# Import standard libraries
import os
import sys
import itertools


# Import third party libraries
import pandas as pd
import numpy as np
import tables as tb

import networkx as nx

import scipy.cluster as cls
import scipy.spatial as spt
import scipy.stats as sts

from scipy.sparse import csr_matrix
from scipy.sparse.csgraph import dijkstra
from scipy.sparse.csgraph import connected_components



###
###  Set variables as needed
###

#Set variables
data_path = sys.argv[1]
file_name = sys.argv[2]


deflect_distance = 2.5


###
###  Define functions as needed
###

#Calculate root sum square distance between two n-dimensional containers
def distance(zipped_coords):
    '''Return the distance between n-dimensional points
       using root sum squares.
    '''
    coord_squares = [(c1 - c2)**2 for c1, c2 in zipped_coords]
    return np.sqrt(sum(coord_squares))


def end_df(pieces, short_version=True, connected=False):
  '''
  Provide information for each end point of each piece in 'pieces'.

  input:  list of lists containing n-dimensional points.
  output: pandas dataframe 


  The returned dataframe includes the following columns:

  Piece_Index - Each piece in 'pieces' is given a sequential number
  End_Index - The first point in a piece is 0, the final point is 1
  End_Point - Coordinates of the point
  Near_Point - Coordinates of the nearest point not on the same piece
  NP_Piece - Piece_Index of the piece containing the nearest point
  NP_Index - Index in the NP_Piece at which the Near_Point is located
  End_Distance - Distance between End_Point and Near_Point
  Piece_Length - Length along the piece


  If True, short_version returns only information for the nearest point
  to each end. If False, the dataframe contains information about the
  distance from each end to the nearest point of each other piece.

  If True, connected returns 
  '''
  #Initialize dataframe
  end_info_df = pd.DataFrame()

  #Compare each piece to all other pieces
  for piece_index, nearpiece_index in itertools.permutations(range(len(pieces)),int(2)):

    this_piece = pieces[piece_index]

    #End coordinates of the piece in question
    end_1 = pieces[piece_index][0]
    end_2 = pieces[piece_index][-1]

    #Calculate distance between each end and all points of one other piece
    #dist_list has two entries; the first is a list of distances to end_1
    #  and the second is a list of distances to end_2
    dist_list = spt.distance.cdist([end_1,end_2],pieces[nearpiece_index])
    #near_points is generated from the smallest distance from each of the
    # two lists and returning the distance and index
    near_points = [sorted(enumerate(each), key=lambda i: i[1])[0] for each in dist_list]

    #For each end, determine minimum distance from end to the other piece
    #Also record how far away the other end is at its nearest point
    for end_index, each_end in enumerate(near_points):
      #Returns 0th entry for first point and -1st for last point
      end_point = this_piece[-end_index]
      nearpoint_index = each_end[0]
      nearpoint = pieces[nearpiece_index][nearpoint_index]
      end_distance = each_end[1]
      piece_length = sum([distance(zip(this_piece[index-1],this_piece[index])) for index in range(len(this_piece))][1:])

      connection_df = pd.DataFrame(
  data=[[piece_index,end_index,end_point,nearpoint,nearpiece_index,nearpoint_index,end_distance,piece_length]],
  columns=['Piece_Index','End_Index','End_Point','Near_Point','NP_Piece','NP_Index','End_Distance','Piece_Length'])
      end_info_df = pd.concat([end_info_df, connection_df])

  #We only care about the closest points
  if len(end_info_df) == 0:
    return end_info_df
  if not connected:
    end_info_df = end_info_df[end_info_df.End_Distance > 0]
  if short_version:
    end_info_df = end_info_df.sort_values('End_Distance', ascending=True).groupby(['Piece_Index','End_Index']).head(int(1))
  elif not short_version:
    end_info_df = end_info_df.sort_values(['Piece_Index','End_Index','End_Distance'])

  #Sort and reset indexes
  end_info_df.sort_values(['Piece_Index','End_Index'], inplace=True)
  end_info_df.reset_index(drop=True, inplace=True)

  return end_info_df



def find_end_pairs(end_info_df):
  '''
  The Piece_Index and End_Index values are used to identify contiguous ends.

  input:  pandas dataframe generated by end_df() function with connected=True
  output: input dataframe with additional columns identifying end connections

  The returned dataset includes the following additional columns:

  This_End_Info - Combine Piece_Index and End_Index into a tuple for comparison
  End_Pair_Info - False if not connected, otherwise (piece,index) of connection
  '''
  #Combine the Piece_Index and End_Index columns into a tuple
  this_end = end_info_df.apply(lambda i: (i.Piece_Index, i.End_Index), axis=1)
  this_end.name = 'This_End_Info'
  end_info_df = end_info_df.merge(this_end, left_index=True, right_index=True)

  #Do the same for the nearpoint if it is connected.
  end_pair = []

  for each_end in end_info_df.itertuples():
    if each_end.End_Distance == 0:
      if each_end.NP_Index == 0:
        end_pair.append((each_end.NP_Piece,0))
      elif each_end.NP_Index > 0:
        end_pair.append((each_end.NP_Piece, 1))
    else:
      end_pair.append(False)

  end_info_df = end_info_df.merge(pd.Series(end_pair, name='End_Pair_Info'), left_index=True, right_index=True)
  return end_info_df


def find_nodes(end_info_df):
  '''
  Contiguous end information is used to label nodes. Each end is a node and the
  piece of that row connects one node to another at the other end.

  input:  pandas dataframe returned by find_end_pairs() function
  output: input dataframe with additional columns identifying node connections

  The returned dataframe includes the following additional columns:

  Node       - Node of end for this row
  Connected_Node - Node of the other end for the piece for this row
  '''
  #The index must be reset for the function to work as expected
  end_info_df.reset_index(drop=True, inplace=True)

  #The first node is always zero
  node_list = [0]

  for index, each_end in enumerate(end_info_df.itertuples()):
    #Always start with a new node
    if index == 0:
      continue

    #If the end for this row has already been assigned a node, use the previous assignment
    if (each_end.This_End_Info in list(end_info_df.This_End_Info.head(int(index)).values)):
      node_id = end_info_df[end_info_df.This_End_Info == each_end.This_End_Info].index.values[0]
      node_num = node_list[node_id]
      node_list.append(node_num)

    #If the end for this row is not connected to any other ends, it gets a new node designation
    elif (not each_end.End_Pair_Info):
      node_list.append(max(node_list) + 1)

    #If the end for this row has a connection, several conditions must be met before making a new node
    #If the end is the connection point of an end already examined, use that node
    elif (each_end.This_End_Info in list(end_info_df.End_Pair_Info.head(int(index)).values)):
      node_id = end_info_df[end_info_df.End_Pair_Info == each_end.This_End_Info].index.values[0]
      node_num = node_list[node_id]
      node_list.append(node_num)

    #If the end to which this end is connected has already been examined
    elif (each_end.End_Pair_Info in list(end_info_df.This_End_Info.head(int(index)).values)):
      node_id = end_info_df[end_info_df.This_End_Info == each_end.End_Pair_Info].index.values[0]
      node_num = node_list[node_id]
      node_list.append(node_num)

    #If the end to which this end is connected is already a part of an assigned
    elif (each_end.End_Pair_Info in list(end_info_df.End_Pair_Info.head(int(index)).values)):
      node_id = end_info_df[end_info_df.End_Pair_Info == each_end.End_Pair_Info].index.values[0]
      node_num = node_list[node_id]
      node_list.append(node_num)

    #If there are more than three pieces connected, check the end point
    elif (each_end.End_Point in list(end_info_df.End_Point.head(int(index)).values)):
      node_id = end_info_df[end_info_df.End_Point == each_end.End_Point].index.values[0]
      node_num = node_list[node_id]
      node_list.append(node_num)

    else:
      node_list.append(max(node_list) + 1)

  end_info_df = end_info_df.merge(pd.Series(node_list, name='Node'), left_index=True, right_index=True)

  #Determine which other nodes you can get to from each end
  #We can get this by just flipping the End_Index order for each piece
  connected_node_list = list(end_info_df.sort_values(['Piece_Index','End_Index'], ascending=[True, False]).Node.values)

  end_info_df = end_info_df.merge(pd.Series(connected_node_list, name='Connected_Node'), left_index=True, right_index=True)

  return end_info_df


def graphify(end_info_df):
  '''
  Produces a graph using the end_info_df nodes. The Piece_Length information is
  used to calculate the distance along all paths between each pair of nodes.

  input:  pandas dataframe that has been processed with find_nodes() function
  output: tuple of path data in matrix form

  Returned tuple of two matrixes. Columns and rows correspond to node numbers:

  path distances - entries correspond to node-to-node path distance
  predecessors   - entries correspond to previous node index along shortest path
  '''
  #Python starts counting at 0
  number_of_nodes = end_info_df.Node.max() + 1

  #Use piece lengths to create distance graph
  graph_mat = np.zeros((number_of_nodes,number_of_nodes))
  for each_end in end_info_df.itertuples():
    graph_mat[each_end.Node][each_end.Connected_Node] = each_end.Piece_Length

  #Functions taken from scipy.sparse
  condensed_mat = csr_matrix(graph_mat)
  path_distances, predecessors = dijkstra(condensed_mat, return_predecessors=True)

  return path_distances, predecessors


def groupify(end_info_df):
  '''
  Contiguous end information is used to label groups. A group is a collection of
  pieces connected in the sense that each piece shares a point with at least
  one other piece in the group and no points with any piece not in the group.

  input:  pandas dataframe that has been processed with find_nodes() function
  output: input dataframe with additional columns identifying connective groups

  The returned dataframe includes the following additional columns:

  Group  - Group of end for this row
  NP_Group - Group of the nearest point to this end
  '''
  #Python starts counting at 0
  number_of_nodes = end_info_df.Node.max() + 1

  #Use piece lengths to create distance graph
  graph_mat = np.zeros((number_of_nodes,number_of_nodes))
  for each_end in end_info_df.itertuples():
    graph_mat[each_end.Node][each_end.Connected_Node] = each_end.Piece_Length

  #Functions taken from scipy.sparse
  condensed_mat = csr_matrix(graph_mat)
  _, labels = connected_components(condensed_mat, directed=False)

  groups = list(zip(np.transpose(np.where(graph_mat)),labels))
  group_ids = []

  for each_end in end_info_df.itertuples():
    group_ids.append(labels[each_end.Node])

  end_info_df = end_info_df.merge(pd.Series(group_ids, name='Group'), left_index=True, right_index=True)

  NP_Group = []
  for each_end in end_info_df.itertuples():
    NP_Group.append(end_info_df[end_info_df.Piece_Index == each_end.NP_Piece].Group.values[0])

  end_info_df = end_info_df.merge(pd.Series(NP_Group, name='NP_Group'), left_index=True, right_index=True)

  return end_info_df


###
###  Load relevant information
###

piece_df = pd.read_hdf(data_path + 'pieces/' + file_name)


#Initialize a dataframe to store results
file_df = pd.DataFrame()

if len(piece_df) == 0:
  sys.exit("Empty piece file.")


event_list = piece_df.Event.unique()


for event_number in event_list:
#for event_number in [233]:
  #Retrieve the pieces for this event and sort them by length
  pieces = piece_df[piece_df.Event == event_number].Key_Points.values
  if len(pieces) == 0:
    continue
  pieces = sorted(pieces, key=lambda i: len(i))
  pieces = [each for each in pieces if not len(each) == 0]

  piece_lengths = [sum([distance(zip(each_piece[index-1],each_piece[index])) for index in range(len(each_piece))][1:]) for each_piece in pieces]

  #If there is a single path, record information and move to the next event
  if len(pieces) == 1:
    End_A = pieces[0][0]
    End_B = pieces[0][-1]
    path_length = piece_lengths[0]
    path_deflections = [0]
    path_pieces = (0)
    end_indicators = (0)
    piece_lengths = [path_length]

    all_paths = pd.DataFrame(data=[[path_length, path_deflections, path_pieces, end_indicators, piece_lengths, End_A, End_B, event_number]], columns=['Path_Length','Path_Deflections','Path_Pieces','End_Indicators','Piece_Lengths','End_A','End_B','Event'])
    file_df = file_df.append(all_paths)

    continue


  #Clean pieces
  all_ends = end_df(pieces, short_version=True, connected=True)
  all_ends = find_end_pairs(all_ends)
  all_ends = find_nodes(all_ends)
  all_ends = groupify(all_ends)


  #All pieces files should be completely connected;
  if len(all_ends[all_ends.Group != all_ends.NP_Group]) > 0:
    print(f'Multiple groups for event {event_number} in {file_name}')
    continue
#    unique_ends = list(set(all_ends[all_ends.Group != all_ends.NP_Group].End_Point.values))
#    pieces.extend([unique_ends])

#    all_ends = end_df(pieces, short_version=True, connected=True)
#    all_ends = find_end_pairs(all_ends)
#    all_ends = find_nodes(all_ends)
#    all_ends = groupify(all_ends)


  path_distances, predecessors = graphify(all_ends)

  #Find ends and junctions
  end_nodes = all_ends[all_ends.End_Pair_Info == False].Node.values
  connect_count = all_ends.groupby('Node').count().sort_values(['Piece_Index'], ascending=False)
  junctions = connect_count[connect_count.Piece_Index >= 3].index.values


  #Look at each junction and calculate the different combinations of angles passing through that junction
  junction_info = {}

  for j_number in junctions:
    j_combos = all_ends[all_ends.Node == j_number][['Piece_Index','End_Index','End_Point','Connected_Node']]
    j_combos.reset_index(drop=True, inplace=True)

    dir_vecs = []

    #Create a direction vector for each piece entering the junction
    for piece_info in j_combos.itertuples():
      #The point into which multiple pieces enter
      junction_point = piece_info.End_Point

      #The piece relevant to this row
      piece = pieces[piece_info.Piece_Index][::(-1)**piece_info.End_Index]

      #Calculate the keypoint to keypoint distance along the piece
      pt_dists = [distance(zip(each,piece[index-1])) for index,each in enumerate(piece)][1:]

      #Select points along the piece that are more than deflect_distance from the junction
      # The first point appearing after this is selected for calculating junction deflections
      if sum(pt_dists) > deflect_distance:
        cum_dists = np.cumsum(pt_dists)
        line_pt_info = [each for each in enumerate(cum_dists) if each[1] > deflect_distance][0]
        line_pt = piece[line_pt_info[0] +1]

      #If the entire piece is less than deflect_distance long, the final point along the piece is used
      else:
        line_pt = piece[-1]


      #Create a vector by subtracting the junction point from the point determined above
      this_vec = np.array(line_pt) - np.array(junction_point)

      #Make this a normal vector by dividing by its own length
      vec_len = distance(zip((0,0,0),this_vec))
      if vec_len > 0:
        this_vec /= vec_len

      #Record this vector
      dir_vecs.append(this_vec)

    #Add this information to the combinations dataframe
    dir_vecs = pd.Series(dir_vecs, name='Dir_Vec')
    j_combos = pd.concat((j_combos,dir_vecs), axis=int(1))


    j_paths = {}

    #Look at each combination of directions through the junction and calculate the deflection
    for end_combo in list(itertools.combinations(j_combos.Connected_Node.values,2)):
      vec_1 = j_combos[j_combos.Connected_Node == end_combo[0]].Dir_Vec.values[0]
      vec_2 = j_combos[j_combos.Connected_Node == end_combo[1]].Dir_Vec.values[0]

      dot = np.inner(vec_1,vec_2)
      deflec = np.arccos(dot)/np.pi

      j_paths[end_combo] = deflec

    junction_info[j_number] = j_paths


  #Create a new dataframe out of paths from one loose end to another
  possible_paths = []
  path_lengths = []
  j_list = []

  #Create a graph for path analysis
  event_graph = nx.Graph({each_node[0]:dict(zip(each_node[1].Connected_Node.values, each_node[1].Piece_Length.values)) for each_node in all_ends.groupby('Node')}, loops=True, weighted=True)


  for start_node, end_node in list(itertools.combinations(end_nodes,2)):
    try:
      path = list(nx.shortest_simple_paths(event_graph,start_node,end_node))[-1]
    except:
      break
    possible_paths.append(tuple(path))

    path_junctions = list(set(junctions).intersection(path))
    j_list.append(path_junctions)

    path_lengths.append(path_distances[start_node][end_node])


  #If no paths can be made between ends, go to next event
  if len(possible_paths) == 0:
    continue


  #Make a dataframe from the path data
  all_paths = pd.DataFrame(data = np.array([possible_paths, j_list, path_lengths], dtype=object).T, columns=['Path','Path_Junctions','Path_Length'])


  #Make a list of all of the deflection angles along each path
  all_deflections = []

  for each_path in all_paths.itertuples():
    this_path = each_path.Path
    path_deflections = []

    #Identify the ends of the path and the junctions along it
    path_ends = (each_path.Path[0], each_path.Path[-1])
    true_junctions = [each for each in each_path.Path_Junctions if not each in path_ends]

    #Identify the three points relevant for each junction
    for each_junction in true_junctions:
      junction_index = this_path.index(each_junction)
      near_junction_points = (this_path[junction_index-1],this_path[junction_index+1])

      #Junction info key tuples are only stored in one orientation
      try:
        deflect = junction_info[each_junction][near_junction_points]
      except:
        deflect = junction_info[each_junction][near_junction_points[::-1]]

      path_deflections.append(deflect)

    all_deflections.append(path_deflections)

  if len(all_deflections) > 0:
    all_paths = all_paths.merge(pd.Series(all_deflections, name='Path_Deflections'), left_index=True, right_index=True)
  else:
    all_paths['Path_Deflections'] = [0]


  #The all_paths dataframe uses node notation. Piece number notation is more useful for reconstruction
  path_series = []
  recon_series = []
  indicator_series = []
  length_series = []

  for each_path in all_paths.Path:
    path_pieces = []
    path_reconstruction = []
    end_indicators = []
    part_lengths = []

    for node_index in range(len(each_path) -1):
      this_node = each_path[node_index]
      next_node = each_path[node_index +1]

      #Identify a node to node connection
      no_duplicates = ~all_ends.Piece_Index.isin(path_pieces)
      start_connection = all_ends.Node == this_node
      end_connection = all_ends.Connected_Node == next_node
      possible_pieces_df = all_ends[no_duplicates & start_connection & end_connection].sort_values('Piece_Length')

      #Some node to node paths may have multple pieces in the event of looping.  Select the shortest
      piece_number = possible_pieces_df.Piece_Index.values[0]
      direction_indicator = possible_pieces_df.End_Index.values[0]

      end_indicators.append(direction_indicator)
      path_pieces.append(piece_number)
      path_reconstruction.extend(pieces[piece_number][::(-1)**(direction_indicator)])
      part_lengths.append(piece_lengths[piece_number])

    path_series.append(tuple(path_pieces))
    indicator_series.append(tuple(end_indicators))
    recon_series.append(path_reconstruction)
    length_series.append(part_lengths)

  path_series = pd.Series(path_series)
  indicator_series = pd.Series(indicator_series)
  recon_series = pd.Series(recon_series)
  length_series = pd.Series(length_series)

  path_series.name = 'Path_Pieces'
  indicator_series.name = 'End_Indicators'
  recon_series.name = 'Full_Path'
  length_series.name = 'Piece_Lengths'

  all_paths = all_paths.merge(path_series, left_index=True, right_index=True)
  all_paths = all_paths.merge(indicator_series, left_index=True, right_index=True)
  all_paths = all_paths.merge(recon_series, left_index=True, right_index=True)
  all_paths = all_paths.merge(length_series, left_index=True, right_index=True)


  all_paths = all_paths[all_paths.Full_Path.apply(len) > 0]
  all_paths.reset_index(drop=True, inplace=True)

  #Find the ends from the reconstruction
  End_A = all_paths.Full_Path.apply(lambda i: i[0])
  End_A.name = 'End_A'

  End_B = all_paths.Full_Path.apply(lambda i: i[-1])
  End_B.name = 'End_B'

  all_paths = all_paths.merge(End_A, left_index=True, right_index=True)
  all_paths = all_paths.merge(End_B, left_index=True, right_index=True)


  all_paths['Event'] = event_number

  all_paths.drop(columns=['Path','Path_Junctions','Full_Path'], inplace=True)

  file_df = file_df.append(all_paths)



#Reset indexes
file_df = file_df.reset_index(drop=True)


#Make a folder for the current configuration
root_dir = data_path + '/paths/' #.replace('.','_')



if not os.path.exists(root_dir):
    os.mkdir(root_dir)


#Save the ends file
save_dir = root_dir #+ 'ends/'
filepath = save_dir + file_name

if not os.path.exists(save_dir):
    os.mkdir(save_dir)

file_df.to_hdf(filepath, 'ABR')
